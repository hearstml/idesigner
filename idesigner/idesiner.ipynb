{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designers Image Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load external libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/hhan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda, ELU\n",
    "from keras.layers.convolutional import Convolution2D, Cropping2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designers Classification\n",
    "\n",
    "**Steps**:\n",
    "1. Data preparation:\n",
    "    - Load the images and labels\n",
    "    - Split the data into two sets:  training set and testing set\n",
    "2. Build a Convolutional Neural Network model (CNN). \n",
    "3. Train the model with the training data and test the accuracy using testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:  Load the images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vera wang\n",
      "marc by marc jacobs\n",
      "carolina herrera\n",
      "calvin klein\n",
      "blumarine\n",
      "missoni\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hotdog import load_images, shuffle_samples, load_image_with_label\n",
    "import pandas as pd \n",
    "\n",
    "image_size = (114, 114)\n",
    "\n",
    "# for each image, we will produce one rotated image along with the original image\n",
    "n_rotated = 1\n",
    "# number of class, 1 is hotdog, 0 is not hotdog\n",
    "n_classes = 2\n",
    "\n",
    "# get all the hotdog and not hotdog images paths\n",
    "designers = [i for i in os.listdir(\"/Users/hhan/fashion/designer_image\") if not i.startswith(\".\")]\n",
    "image_dir = \"/Users/hhan/fashion/designer_image\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for i in range(len(designers)):\n",
    "    print (designers[i])\n",
    "    designer_dir = os.path.join(image_dir, designers[i])\n",
    "    designer_images = [os.path.join(designer_dir, i) for i in os.listdir(designer_dir) if not i.startswith(\".\")]\n",
    "    \n",
    "    hotdog_images, hotdog_labels = load_image_with_label(designer_images, i, image_size, n_rotated)\n",
    "    images.append(hotdog_images)\n",
    "    labels.append(hotdog_labels)\n",
    "    \n",
    "all_images = np.concatenate(images, axis=0)\n",
    "all_labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10290, 114, 114, 3)\n",
      "(10290,)\n"
     ]
    }
   ],
   "source": [
    "# images are all the images combining hotdog and not hotdog images\n",
    "print (all_images.shape)\n",
    "print (all_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset  into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples 9261\n",
      "number of testing samples 1029\n"
     ]
    }
   ],
   "source": [
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.1, random_state=rand_state)\n",
    "print(\"number of training samples\", np.array(X_train).shape[0])\n",
    "print(\"number of testing samples\", np.array(X_test).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 2 : Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kerasModel(inputShape):\n",
    "    # feature learning \n",
    "    model = Sequential()\n",
    "    \n",
    "    # convolutional block 1 \n",
    "    model.add(Convolution2D(32, (5, 5), input_shape=inputShape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    # convolutional block 2\n",
    "    model.add(Convolution2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   \n",
    "    # convolutional block 3\n",
    "    model.add(Convolution2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # classification\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.compile('adam', 'binary_crossentropy', ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Step 3 : Train and evalutate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8334 samples, validate on 927 samples\n",
      "Epoch 1/10\n",
      "8334/8334 [==============================] - 99s 12ms/step - loss: -19.6406 - acc: 0.1955 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 2/10\n",
      "8334/8334 [==============================] - 96s 12ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 3/10\n",
      "8334/8334 [==============================] - 113s 14ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 4/10\n",
      "8334/8334 [==============================] - 114s 14ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 5/10\n",
      "8334/8334 [==============================] - 109s 13ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 6/10\n",
      "8334/8334 [==============================] - 112s 13ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 7/10\n",
      "8334/8334 [==============================] - 111s 13ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 8/10\n",
      "8334/8334 [==============================] - 109s 13ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 9/10\n",
      "8334/8334 [==============================] - 112s 13ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "Epoch 10/10\n",
      "8334/8334 [==============================] - 117s 14ms/step - loss: -19.8849 - acc: 0.1958 - val_loss: -20.6890 - val_acc: 0.1899\n",
      "1029/1029 [==============================] - 5s 4ms/step\n",
      "loss: -19.76917790810499\n",
      "acc: 0.18658892131176125\n"
     ]
    }
   ],
   "source": [
    "inputShape = (114, 114, 3)\n",
    "model = kerasModel(inputShape)\n",
    "history = model.fit(X_train, y_train, nb_epoch=10, validation_split=0.1)\n",
    "\n",
    "metrics = model.evaluate(X_test, y_test)\n",
    "for metric_i in range(len(model.metrics_names)):\n",
    "    metric_name = model.metrics_names[metric_i]\n",
    "    metric_value = metrics[metric_i]\n",
    "    print('{}: {}'.format(metric_name, metric_value))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
